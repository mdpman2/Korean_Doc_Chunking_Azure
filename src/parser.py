import os
import base64
from io import BytesIO
from PIL import Image
from pdf2image import convert_from_path
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeResult
from azure.core.credentials import AzureKeyCredential
from openai import AzureOpenAI
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class HybridDocumentParser:
    def __init__(self):
        self.di_client = DocumentIntelligenceClient(
            endpoint=os.getenv("AZURE_DI_ENDPOINT"),
            credential=AzureKeyCredential(os.getenv("AZURE_DI_KEY"))
        )
        self.aoai_client = AzureOpenAI(
            api_key=os.getenv("AZURE_OPENAI_KEY"),
            api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
        )
        self.gpt_model = os.getenv("AZURE_OPENAI_DEPLOYMENT")

    def _encode_image_base64(self, pil_image):
        buffered = BytesIO()
        pil_image.save(buffered, format="JPEG")
        return base64.b64encode(buffered.getvalue()).decode('utf-8')

    def _describe_image(self, pil_image, image_idx):
        """GPT-4oë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì˜ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì¶”ì¶œ"""
        print(f"   ğŸ¤– GPT-4o Analyzing Figure #{image_idx}...")
        base64_img = self._encode_image_base64(pil_image)

        system_prompt = (
            "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì´ë¯¸ì§€(ì°¨íŠ¸, í‘œ, ë‹¤ì´ì–´ê·¸ë¨ ë“±)ë¥¼ ë³´ê³  "
            "RAG(ê²€ìƒ‰ ì¦ê°• ìƒì„±) ì‹œìŠ¤í…œì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”. "
            "ë‹¨ìˆœí•œ ì‹œê°ì  ë¬˜ì‚¬ë³´ë‹¤ëŠ”, 'ë°ì´í„°ì˜ ìˆ˜ì¹˜', 'ì¶”ì„¸', 'í•µì‹¬ ë©”ì‹œì§€'ë¥¼ í•œêµ­ì–´ë¡œ ëª…í™•íˆ ì„œìˆ í•˜ì„¸ìš”."
        )

        try:
            response = self.aoai_client.chat.completions.create(
                model=self.gpt_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": [
                        {"type": "text", "text": "ì´ ì´ë¯¸ì§€ì˜ ë‚´ìš©ì„ ìƒì„¸íˆ ì„¤ëª…í•´ì¤˜."},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_img}"}}
                    ]}
                ],
                temperature=0.0
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"   âŒ Error analyzing image: {e}")
            return "[ì´ë¯¸ì§€ ë¶„ì„ ì‹¤íŒ¨]"

    def parse(self, file_path):
        """
        íŒŒì¼ í˜•ì‹(PDF, PPTX, DOCX)ì— ë”°ë¼ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì‹± ìˆ˜í–‰
        - PDF: Azure DI + Visual RAG (ì´ë¯¸ì§€ í¬ë¡­ & ì„¤ëª…)
        - PPTX/DOCX: Azure DI (í…ìŠ¤íŠ¸/í‘œ íŒŒì‹±) - ì´ë¯¸ì§€ ì„¤ëª…ì€ ìŠ¤í‚µ (Pure Python í•œê³„)
        """
        file_ext = os.path.splitext(file_path)[1].lower()
        print(f"ğŸš€ Parsing started: {file_path} ({file_ext})")

        # 1. Image ë³€í™˜ (PDFì¸ ê²½ìš°ë§Œ)
        page_images = None
        if file_ext == '.pdf':
            try:
                # poppler í•„ìš”
                page_images = convert_from_path(file_path, dpi=200)
            except Exception as e:
                print(f"   âš ï¸ PDF Image conversion failed (Visual RAG will be skipped): {e}")

        # 2. Azure Document Intelligence ì‹¤í–‰
        # PDF, PPTX, DOCX, HTML ë“± ë‹¤ì–‘í•œ í¬ë§· ì§€ì›
        with open(file_path, "rb") as f:
            poller = self.di_client.begin_analyze_document(
                model_id="prebuilt-layout",
                body=f,
                content_type="application/octet-stream",
                output_content_format="markdown"  # Markdown ì¶œë ¥ ìš”ì²­
                # locale="ko-KR", # Document Intelligence ì—ì„œëŠ” localeì´ ëª¨ë¸ ì˜µì…˜ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ. prebuilt-layoutì—ì„  ë³´í†µ ìë™ ê°ì§€.
            )
        result = poller.result()

        full_markdown = result.content
        descriptions = []

        # 3. Figure(ì´ë¯¸ì§€/ì°¨íŠ¸) ê°ì§€ ë° GPT-4o ì²˜ë¦¬ (PDFì´ê³  ì´ë¯¸ì§€ê°€ ë³€í™˜ëœ ê²½ìš°ì—ë§Œ)
        if result.figures and page_images:
            print(f"ğŸ“Š Found {len(result.figures)} figures. Starting vision analysis...")

            for idx, figure in enumerate(result.figures):
                if not figure.bounding_regions: continue

                region = figure.bounding_regions[0]
                page_num = region.page_number - 1

                # í˜ì´ì§€ ë²”ìœ„ ì²´í¬
                if page_num >= len(page_images):
                    continue

                page_img = page_images[page_num]
                # di_page = result.pages[page_num] # New SDK might handle pages differently, checking structure.
                # In new SDK, result.pages is a list of DocumentPage
                di_page = result.pages[page_num] # Assuming page index matches

                # ì¢Œí‘œ ìŠ¤ì¼€ì¼ë§
                polygon = region.polygon
                x_coords = [p.x for p in polygon]
                y_coords = [p.y for p in polygon]

                # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
                if di_page.width == 0 or di_page.height == 0:
                    continue

                scale_x = page_img.width / di_page.width
                scale_y = page_img.height / di_page.height

                left = min(x_coords) * scale_x
                top = min(y_coords) * scale_y
                right = max(x_coords) * scale_x
                bottom = max(y_coords) * scale_y

                # ì´ë¯¸ì§€ í¬ë¡­
                try:
                    cropped_img = page_img.crop((left, top, right, bottom))
                    # ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ëŠ” ìŠ¤í‚µ (ì„ íƒì‚¬í•­)
                    if cropped_img.width < 50 or cropped_img.height < 50:
                        continue

                    # GPT-4o ë¶„ì„
                    desc_text = self._describe_image(cropped_img, idx + 1)

                    insertion_block = f"\n\n> **[ì´ë¯¸ì§€/ì°¨íŠ¸ ì„¤ëª… {idx+1}]**\n> {desc_text}\n\n"

                    offset = figure.spans[0].offset if figure.spans else len(full_markdown)
                    descriptions.append((offset, insertion_block))

                except Exception as e:
                    print(f"   âš ï¸ Error cropping/analyzing figure {idx+1}: {e}")

        elif result.figures and not page_images:
             print(f"â„¹ï¸ Figures detected but Visual RAG skipped for non-PDF format: {file_ext}")

        # 4. ì„¤ëª… í…ìŠ¤íŠ¸ ë³‘í•©
        descriptions.sort(key=lambda x: x[0], reverse=True)

        for offset, text in descriptions:
            if offset <= len(full_markdown):
                full_markdown = full_markdown[:offset] + text + full_markdown[offset:]
            else:
                full_markdown += text

        print("âœ… Parsing completed.")
        return full_markdown
