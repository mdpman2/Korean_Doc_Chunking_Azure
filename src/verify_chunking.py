import os
import sys
from dotenv import load_dotenv

# Add root to path
sys.path.append(os.getcwd())

# Load env before importing chunker which might use os.environ in global scope if any
load_dotenv()

from src.chunker import optimized_korean_chunking
from langchain_core.documents import Document

def test_chunking_real():
    print("ðŸ§ª Testing Real Semantic Chunking (requires valid .env)...")

    # Mock markdown text
    text = """
# ì„¸ê¸ˆ ì œë„ ê°œíŽ¸

2024ë…„ë¶€í„° ì„¸ê¸ˆ ì œë„ê°€ í¬ê²Œ ê°œíŽ¸ë©ë‹ˆë‹¤.
ì†Œë“ì„¸ìœ¨ì´ ì¡°ì •ë˜ë©°, ê³µì œ í•­ëª©ì´ í™•ëŒ€ë©ë‹ˆë‹¤.

## ì†Œë“ì„¸
ì†Œë“ì„¸ ê³¼ì„¸í‘œì¤€ êµ¬ê°„ì´ ìƒí–¥ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤.
ì´ëŠ” ë¬¼ê°€ ìƒìŠ¹ë¥ ì„ ë°˜ì˜í•œ ì¡°ì¹˜ìž…ë‹ˆë‹¤.

## ë²•ì¸ì„¸
ë²•ì¸ì„¸ìœ¨ì€ ì¸í•˜ë˜ì—ˆìŠµë‹ˆë‹¤.
ê¸°ì—…ì˜ íˆ¬ìžë¥¼ ì´‰ì§„í•˜ê¸° ìœ„í•¨ìž…ë‹ˆë‹¤.
"""

    try:
        chunks = optimized_korean_chunking(text)
        print("âœ… Chunking Successful!")
        print(f"   Created {len(chunks)} chunks.")
        for i, chunk in enumerate(chunks):
            print(f"   Shape of Chunk #{i+1}: {len(chunk.page_content)} chars")
            print(f"   Snippet: {chunk.page_content[:50]}...")

    except Exception as e:
        print(f"âŒ Chunking Failed: {e}")
        print("   -> Check your .env for AZURE_OPENAI_EMBEDDING_DEPLOYMENT, API_KEY, ENDPOINT, API_VERSION.")

if __name__ == "__main__":
    test_chunking_real()
